import streamlit as st
import pandas as pd
import numpy as np
import folium
from streamlit_folium import st_folium
import math
from datetime import timedelta, datetime
import os # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£ import os ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ os.path.splitext
import re

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ layout ‡πÉ‡∏´‡πâ‡πÄ‡∏ï‡πá‡∏°‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
st.set_page_config(layout="wide")

st.title("‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ß‡∏¥‡πà‡∏á Rider")

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó) ---
uploaded_file = st.sidebar.file_uploader("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (Excel: .xlsx, .xlsm, .xlsb | ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: .csv, .txt)", type=["xlsx", "xlsm", "xlsb", "csv", "txt"])

df_raw = None # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î df_raw ‡πÄ‡∏õ‡πá‡∏ô None ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Time Check Status
def parse_time_check(value):
    # Handle NaN/None explicitly first
    if pd.isna(value):
        return 'not_yet'

    # Check if it's already a datetime object (e.g., if loaded directly from excel as time)
    if isinstance(value, (datetime, pd.Timestamp, pd.Timedelta)):
        return 'checked'

    value_str = str(value).strip().lower()

    if value_str == 'pending':
        return 'pending'

    try:
        # ‡∏•‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤
        dt_obj = pd.to_datetime(value, format='%H:%M:%S', errors='coerce')
        if pd.isna(dt_obj):
            dt_obj = pd.to_datetime(value, format='%H:%M', errors='coerce')
        if pd.isna(dt_obj):
            dt_obj = pd.to_datetime(value, errors='coerce') # ‡∏•‡∏≠‡∏á‡πÅ‡∏ö‡∏ö‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        if pd.isna(dt_obj):
            return 'not_yet' # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 'not_yet'
        return 'checked' # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏î‡πâ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 'checked'
    except Exception:
        return 'not_yet' # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏Å‡πá‡πÄ‡∏õ‡πá‡∏ô 'not_yet'

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (‡πÉ‡∏ä‡πâ Cache ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå)
@st.cache_data(ttl=600)
def load_and_preprocess_data(file_uploader_object):
    if file_uploader_object is None:
        return None

    df_temp = None
    file_extension = os.path.splitext(file_uploader_object.name)[1].lower()

    if file_extension in [".xlsx", ".xlsm"]:
        try:
            df_temp = pd.read_excel(file_uploader_object, sheet_name="Data", engine="openpyxl")
            st.success(f"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel ({file_extension}) ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        except Exception as e:
            st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel ({file_extension}): {e}")
            return None
    elif file_extension == ".xlsb":
        try:
            # Requires 'pyxlsb' to be installed: pip install pyxlsb
            df_temp = pd.read_excel(file_uploader_object, sheet_name="Data", engine="pyxlsb")
            st.success(f"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel Binary ({file_extension}) ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        except ImportError:
            st.error("‚ùó ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ 'pyxlsb' ‡πÇ‡∏î‡∏¢‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á: `pip install pyxlsb` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå .xlsb")
            return None
        except Exception as e:
            st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel Binary ({file_extension}): {e}")
            return None
    elif file_extension in [".csv", ".txt"]:
        encodings_to_try = ['utf-8', 'tis-620', 'cp1252', 'latin1']
        for encoding in encodings_to_try:
            file_uploader_object.seek(0) # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï pointer ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏£‡∏±‡πâ‡∏á
            try:
                df_temp = pd.read_csv(file_uploader_object, encoding=encoding)
                st.success(f"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ({file_extension}) ‡∏î‡πâ‡∏ß‡∏¢ encoding '{encoding}' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                break # ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å loop ‡∏ñ‡πâ‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
            except UnicodeDecodeError:
                continue # ‡∏•‡∏≠‡∏á encoding ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
            except Exception as e:
                st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ({file_extension}) ‡∏î‡πâ‡∏ß‡∏¢ encoding '{encoding}': {e}")
                return None
        if df_temp is None:
            st.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ({file_extension}) ‡∏î‡πâ‡∏ß‡∏¢ encoding ‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö encoding ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö")
            return None
    else:
        st.error(f"‚ö†Ô∏è ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå '{file_extension}' ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö .xlsx, .xlsm, .xlsb, .csv, ‡∏´‡∏£‡∏∑‡∏≠ .txt")
        return None

    # --- ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ---
    required_columns = ["Order ID", "LAT", "LON", "SLA STS", "Rider Name", "Time Check", "DP Time", "SLA"]
    missing_columns = [col for col in required_columns if col not in df_temp.columns]

    if missing_columns:
        st.error(f"‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î: {', '.join(missing_columns)}")
        return None # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ None ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡πà‡∏≠

    # --- ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
    merged_df_processed = df_temp.copy()
    merged_df_processed["LAT"] = pd.to_numeric(merged_df_processed["LAT"], errors="coerce")
    merged_df_processed["LON"] = pd.to_numeric(merged_df_processed["LON"], errors="coerce")
    merged_df_processed.dropna(subset=["LAT", "LON"], inplace=True)

    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå DP Time ‡πÅ‡∏•‡∏∞ SLA
    for col_name in ['DP Time', 'SLA']:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô DataFrame ‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        if col_name in merged_df_processed.columns:
            if pd.api.types.is_numeric_dtype(merged_df_processed[col_name]):
                merged_df_processed[f'{col_name}_for_sort'] = pd.to_datetime(merged_df_processed[col_name], unit='d', origin='1899-12-30', errors='coerce')
            else:
                merged_df_processed[f'{col_name}_for_sort'] = pd.to_datetime(merged_df_processed[col_name], errors='coerce', format='%H:%M:%S')
                merged_df_processed[f'{col_name}_for_sort'] = merged_df_processed[f'{col_name}_for_sort'].fillna(
                    pd.to_datetime(merged_df_processed[col_name], errors='coerce', format='%H:%M')
                )
            merged_df_processed[col_name] = merged_df_processed[f'{col_name}_for_sort'].dt.strftime('%H:%M').fillna('')
            merged_df_processed = merged_df_processed.drop(columns=[f'{col_name}_for_sort'])
        else:
            merged_df_processed[col_name] = '' # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡πà‡∏≤‡∏á‡πÜ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô CSV ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö


    # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô parse_time_check
    merged_df_processed["Time Check Status"] = merged_df_processed["Time Check"].apply(parse_time_check)
    merged_df_processed["Rider Name"] = merged_df_processed["Rider Name"].astype(str).str.strip()

    # ‡∏Ñ‡πà‡∏≤‡∏û‡∏¥‡∏Å‡∏±‡∏î MFC
    center_lat = 13.737929161400837
    center_lon = 100.63687556823479

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì bearing
    def fast_bearing(lat2, lon2):
        dLon = np.radians(lon2 - center_lon)
        lat1 = np.radians(center_lat)
        lat2 = np.radians(lat2)
        y = np.sin(dLon) * np.cos(lat2)
        x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dLon)
        bearing = np.degrees(np.arctan2(y, x))
        return (bearing + 360) % 360

    merged_df_processed["Bearing"] = fast_bearing(merged_df_processed["LAT"], merged_df_processed["LON"])

    def assign_zone(bearing):
        if 0 <= bearing < 90: return "Zone 4"
        elif 90 <= bearing < 180: return "Zone 3"
        elif 180 <= bearing < 270: return "Zone 2"
        else: return "Zone 1"

    merged_df_processed["Zone"] = merged_df_processed["Bearing"].apply(assign_zone)

    return merged_df_processed, center_lat, center_lon # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏•‡∏∞‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏•‡∏≤‡∏á

# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å Cache
processed_data_tuple = load_and_preprocess_data(uploaded_file)

if processed_data_tuple is not None:
    merged_df, center_lat, center_lon = processed_data_tuple
    st.success("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")

    # --- ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á ---
    st.sidebar.header("‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á")
    filtered_df = merged_df.copy()

    # Filter by Order ID
    order_options = sorted(filtered_df["Order ID"].unique())
    selected_orders = st.sidebar.multiselect("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Order ID:", order_options)
    if selected_orders:
        filtered_df = filtered_df[filtered_df["Order ID"].isin(selected_orders)]

    # Filter by Rider Name
    rider_options = sorted(filtered_df["Rider Name"].unique())
    selected_riders = st.sidebar.multiselect("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Rider Name:", rider_options)
    if selected_riders:
        filtered_df = filtered_df[filtered_df["Rider Name"].isin(selected_riders)]

    # Filter by Time Check (‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå "Time Check" ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á)
    time_check_options = sorted(filtered_df["Time Check"].astype(str).unique().tolist())
    selected_time_checks_raw = st.sidebar.multiselect("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Time Check:", time_check_options)
    if selected_time_checks_raw:
        filtered_df = filtered_df[filtered_df["Time Check"].astype(str).isin(selected_time_checks_raw)]

    # Filter by Zone
    zone_options = sorted(filtered_df["Zone"].unique())
    selected_zones = st.sidebar.multiselect("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Zone:", zone_options)
    if selected_zones:
        filtered_df = filtered_df[filtered_df["Zone"].isin(selected_zones)]


    # --- ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'SLA_for_sort' ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
    # ‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏ô merged_df ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ filtered_df ‡∏≠‡∏≤‡∏à‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
    if "SLA_for_sort" in merged_df.columns:
        filtered_df = filtered_df.sort_values(by=["SLA_for_sort", "Order ID"], ascending=[True, True])
    else:
        filtered_df = filtered_df.sort_values(by="Order ID", ascending=True)

    # --- ‡πÅ‡∏™‡∏î‡∏á Distribution of Time Check Status ‡πÉ‡∏ô Filtered Data ---
    st.sidebar.markdown("---")
    st.sidebar.subheader("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Time Check ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á:")
    if not filtered_df.empty:
        status_counts = filtered_df["Time Check Status"].value_counts()
        for status, count in status_counts.items():
            st.sidebar.write(f"- {status.capitalize()}: {count} orders")
    else:
        st.sidebar.write("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏≠‡∏á")
    st.sidebar.markdown("---")


    # --- ‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå Google Maps ---
    if not filtered_df.empty:
        # ‡πÉ‡∏ä‡πâ .copy() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô SettingWithCopyWarning
        unique_coords_df = filtered_df.groupby(["LAT", "LON"]).size().reset_index()[["LAT", "LON"]].copy()
        destination_coords = [f"{row['LAT']},{row['LON']}" for _, row in unique_coords_df.iterrows()]

        if len(destination_coords) > 11:
            st.warning("‚ö†Ô∏è ‡∏°‡∏µ‡∏à‡∏∏‡∏î‡∏´‡∏°‡∏≤‡∏¢‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 11 ‡∏à‡∏∏‡∏î ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 11 ‡∏à‡∏∏‡∏î‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô")
            destination_coords = destination_coords[:11]

        if destination_coords:
            origin_param = f"{center_lat},{center_lon}"
            path_coords = [origin_param] + destination_coords
            # ‡πÉ‡∏ä‡πâ URL ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á Google Maps ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á
            maps_url = f"https://www.google.com/maps/dir/{'/'.join(path_coords)}?travelmode=driving"

            st.markdown(f"**üìç [‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô Google Maps]({maps_url})**")
    else:
        st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Order ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á")

    # --- ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà ---
    m = folium.Map(location=[center_lat, center_lon], zoom_start=14)
    folium.Marker(location=[center_lat, center_lon], popup="üìçMFC", icon=folium.Icon(color="green", icon="star")).add_to(m)
    folium.Circle(location=[center_lat, center_lon], radius=4000, color='blue', opacity=0.4, fill=True, fill_color='red', fill_opacity=0.05, popup="‡∏£‡∏±‡∏®‡∏°‡∏µ 4 ‡∏Å‡∏°.").add_to(m)

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Zone
    zone_colors = {"Zone 1": "red", "Zone 2": "orange", "Zone 3": "purple", "Zone 4": "blue"}

    # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ö‡∏ô‡∏´‡∏°‡∏∏‡∏î
    grouped = filtered_df.groupby(["LAT", "LON"]).agg(
        order_ids=('Order ID', lambda x: ", ".join(sorted(set(map(str, x))))),
        rider_names=('Rider Name', lambda x: ", ".join(sorted(set(map(str, x))))),
        dp_times=('DP Time', lambda x: ", ".join(sorted(filter(None, x)))),
        sla_times=('SLA', lambda x: ", ".join(sorted(filter(None, x)))),
        zone=('Zone', 'first'),
    ).reset_index()

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Popup Text ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏°‡∏∏‡∏î
    for idx, row in grouped.iterrows():
        popup_html = f"""
        <b>Order:</b> {row['order_ids']}<br>
        <b>Rider:</b> {row['rider_names']}<br>
        <b>DP Time:</b> {row['dp_times'] if row['dp_times'] else 'N/A'}<br>
        <b>SLA:</b> {row['sla_times'] if row['sla_times'] else 'N/A'}<br>
        <b>Zone:</b> {row['zone']}<br>
        """

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏µ‡∏´‡∏°‡∏∏‡∏î‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° Rider Zone
        marker_color = zone_colors.get(row['zone'], 'blue')


        folium.Marker(
            location=[row["LAT"], row["LON"]],
            popup=popup_html,
            icon=folium.Icon(color=marker_color, icon="info-sign")
        ).add_to(m)

    # ‡∏ß‡∏≤‡∏î‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ï‡∏≤‡∏°‡πÇ‡∏ã‡∏ô
    #for zone, color in zone_colors.items():
        #zone_points_df = filtered_df[filtered_df["Zone"] == zone]
        #if not zone_points_df.empty:
            #route_points = [[center_lat, center_lon]] + zone_points_df[["LAT", "LON"]].values.tolist()
            #folium.PolyLine(route_points, color=color, weight=2.5, opacity=0.8, popup=zone).add_to(m)

    st_folium(m, width="100%", height=900)

    # --- ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
    columns_to_display = ["Order ID", "SLA", "Rider Name", "SLA STS", "Time Check", "Time Check Status", "Zone", "DP Time"]
    st.dataframe(filtered_df[columns_to_display], use_container_width=True)
else:
    st.info("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Excel ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô")
